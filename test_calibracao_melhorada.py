# -*- coding: utf-8 -*-
"""
Script para testar a nova calibraÃ§Ã£o de scores de confianÃ§a
Compara scores antigos vs novos com exemplos reais
"""
import asyncio
import sys
from typing import Dict, Any


# SimulaÃ§Ã£o da implementaÃ§Ã£o ANTIGA
async def calcular_score_relevancia_OLD(resultado: str, query: str, resultado_traduzido: str = None) -> float:
    """ImplementaÃ§Ã£o antiga (limitada a 0.8 + 0.15)"""
    if not resultado or not query:
        return 0.0

    texto_para_avaliar = resultado_traduzido if resultado_traduzido else resultado

    # Extrair palavras simples (sem stop words por simplicidade)
    palavras_query = [p for p in query.lower().split() if len(p) >= 3]
    if not palavras_query:
        return 0.0

    texto_lower = texto_para_avaliar.lower()
    query_lower = query.lower()

    # Contar matches
    matches = sum(1 for palavra in palavras_query if palavra in texto_lower)

    # Score base limitado a 0.8
    score_base = (matches / len(palavras_query)) * 0.8

    # Bonus limitado a 0.15
    bonus_phrase = 0.0
    if query_lower in texto_lower:
        bonus_phrase = 0.15
    elif matches == len(palavras_query):
        bonus_phrase = 0.10

    return min(1.0, score_base + bonus_phrase)


def calcular_score_ponderado_OLD(
    resultado: Dict[str, Any],
    query: str,
    score_relevancia: float,
    num_ocorrencias: int,
    confiabilidade_fonte: float
) -> float:
    """ImplementaÃ§Ã£o antiga"""
    # RelevÃ¢ncia 50%
    peso_relevancia = 0.50
    valor_relevancia = min(1.0, score_relevancia)

    # OcorrÃªncias 20% (linear)
    peso_ocorrencias = 0.20
    valor_ocorrencias = min(1.0, num_ocorrencias / 5.0)

    # Fonte 20%
    peso_fonte = 0.20
    valor_fonte = min(1.0, confiabilidade_fonte)

    # TÃ­tulo 10%
    peso_titulo = 0.10
    titulo = resultado.get("titulo", "").lower()
    palavras_query = [p for p in query.lower().split() if len(p) >= 3]
    if palavras_query:
        titulo_matches = sum(1 for p in palavras_query if p in titulo)
        valor_titulo = min(1.0, titulo_matches / len(palavras_query))
    else:
        valor_titulo = 0.0

    score = (
        valor_relevancia * peso_relevancia +
        valor_ocorrencias * peso_ocorrencias +
        valor_fonte * peso_fonte +
        valor_titulo * peso_titulo
    )

    return min(1.0, max(0.0, score))


# SimulaÃ§Ã£o da implementaÃ§Ã£o NOVA
async def calcular_score_relevancia_NEW(resultado: str, query: str, resultado_traduzido: str = None) -> float:
    """Nova implementaÃ§Ã£o melhorada"""
    if not resultado or not query:
        return 0.0

    texto_para_avaliar = resultado_traduzido if resultado_traduzido else resultado

    palavras_query = [p for p in query.lower().split() if len(p) >= 3]
    if not palavras_query:
        return 0.0

    texto_lower = texto_para_avaliar.lower()
    query_lower = query.lower()

    # Contar matches exatos e parciais
    matches = 0
    matches_parciais = 0
    for palavra in palavras_query:
        if palavra in texto_lower:
            matches += 1
        else:
            palavras_texto = texto_lower.split()
            for palavra_texto in palavras_texto:
                if palavra in palavra_texto or palavra_texto in palavra:
                    matches_parciais += 1
                    break

    # Score base atÃ© 0.75
    score_base = (matches / len(palavras_query)) * 0.75

    # Bonus parcial atÃ© 0.10
    bonus_parcial = (matches_parciais / len(palavras_query)) * 0.10

    # Bonus phrase atÃ© 0.25
    bonus_phrase = 0.0
    if query_lower in texto_lower:
        bonus_phrase = 0.25
    elif matches == len(palavras_query):
        bonus_phrase = 0.15

    return min(1.0, score_base + bonus_parcial + bonus_phrase)


def detectar_brasil_NEW(resultado: Dict[str, Any]) -> bool:
    """Nova detecÃ§Ã£o de Brasil"""
    termos_brasil = ["brasil", "brazilian", "brasileiro", "brasileira", ".br", "brasilia", "brasÃ­lia", "gov.br"]
    texto = f"{resultado.get('titulo', '')} {resultado.get('descricao', '')} {resultado.get('fonte_url', '')}".lower()
    return any(termo in texto for termo in termos_brasil)


def expandir_score_NEW(score: float) -> float:
    """Nova funÃ§Ã£o de expansÃ£o melhorada"""
    if score < 0.25:
        return score
    elif score > 0.80:
        return score
    elif score < 0.40:
        return 0.25 + (score - 0.25) * 1.67
    else:
        return 0.50 + (score - 0.40) * 0.875


def calcular_score_ponderado_NEW(
    resultado: Dict[str, Any],
    query: str,
    score_relevancia: float,
    num_ocorrencias: int,
    confiabilidade_fonte: float
) -> float:
    """Nova implementaÃ§Ã£o melhorada"""
    import math

    # RelevÃ¢ncia 55% (aumentado)
    peso_relevancia = 0.55
    valor_relevancia = min(1.0, score_relevancia)

    # OcorrÃªncias 15% (reduzido, sqrt)
    peso_ocorrencias = 0.15
    valor_ocorrencias = min(1.0, math.sqrt(num_ocorrencias) / math.sqrt(5.0))

    # Fonte 20%
    peso_fonte = 0.20
    valor_fonte = min(1.0, confiabilidade_fonte)

    # TÃ­tulo 10%
    peso_titulo = 0.10
    titulo = resultado.get("titulo", "").lower()
    titulo_pt = resultado.get("titulo_pt", "").lower()
    titulo_completo = f"{titulo} {titulo_pt}"

    palavras_query = [p for p in query.lower().split() if len(p) >= 3]
    if palavras_query:
        titulo_matches = sum(1 for p in palavras_query if p in titulo_completo)
        valor_titulo = min(1.0, titulo_matches / len(palavras_query))
    else:
        valor_titulo = 0.0

    # Score base
    score_base = (
        valor_relevancia * peso_relevancia +
        valor_ocorrencias * peso_ocorrencias +
        valor_fonte * peso_fonte +
        valor_titulo * peso_titulo
    )

    # Bonus Brasil 20%
    bonus_brasil = 0.0
    if detectar_brasil_NEW(resultado):
        bonus_brasil = score_base * 0.20

    # Score final
    score_final = score_base + bonus_brasil

    # ExpansÃ£o
    score_expandido = expandir_score_NEW(score_final)

    return min(1.0, max(0.0, score_expandido))


# Casos de teste
CASOS_TESTE = [
    {
        "nome": "Caso 1: EducaÃ§Ã£o STEM Brasil (exemplo do usuÃ¡rio)",
        "resultado": {
            "titulo": "a educaÃ§Ã£o steam/stem na educaÃ§Ã£o bÃ¡sica brasileira",
            "descricao": "AnÃ¡lise das polÃ­ticas pÃºblicas de educaÃ§Ã£o STEM no Brasil, incluindo programas de capacitaÃ§Ã£o e formaÃ§Ã£o profissional em ciÃªncia e tecnologia",
            "fonte_url": "https://educacao.gov.br/stem",
            "fonte": "perplexity"
        },
        "query": "polÃ­ticas educacionais STEM educaÃ§Ã£o capacitaÃ§Ã£o formaÃ§Ã£o profissional",
        "num_ocorrencias": 1,
        "confiabilidade_fonte": 0.95,
        "esperado_minimo": 0.70  # Esperamos pelo menos 70
    },
    {
        "nome": "Caso 2: Resultado alemÃ£o sobre STEM",
        "resultado": {
            "titulo": "Mehr DiversitÃ¤t in der MINT-Bildung",
            "titulo_pt": "Mais diversidade na educaÃ§Ã£o em STEM",
            "descricao": "Programas alemÃ£es para aumentar diversidade em STEM",
            "descricao_pt": "Programas alemÃ£es para aumentar diversidade em ciÃªncia, tecnologia, engenharia e matemÃ¡tica",
            "fonte_url": "https://bildung.de/mint",
            "fonte": "perplexity"
        },
        "query": "polÃ­ticas educacionais STEM educaÃ§Ã£o capacitaÃ§Ã£o formaÃ§Ã£o profissional",
        "num_ocorrencias": 1,
        "confiabilidade_fonte": 0.95,
        "esperado_minimo": 0.50  # Sem Brasil, deve ser menor que caso 1
    },
    {
        "nome": "Caso 3: Alta relevÃ¢ncia + Brasil + mÃºltiplas ocorrÃªncias",
        "resultado": {
            "titulo": "PolÃ­ticas pÃºblicas para educaÃ§Ã£o STEM no Brasil",
            "descricao": "O governo brasileiro implementou programas de capacitaÃ§Ã£o profissional em STEM com foco em formaÃ§Ã£o de professores",
            "fonte_url": "https://www.gov.br/educacao",
            "fonte": "perplexity"
        },
        "query": "polÃ­ticas educacionais STEM educaÃ§Ã£o capacitaÃ§Ã£o formaÃ§Ã£o profissional",
        "num_ocorrencias": 3,
        "confiabilidade_fonte": 0.95,
        "esperado_minimo": 0.80  # Deve ser ALTO
    },
    {
        "nome": "Caso 4: Baixa relevÃ¢ncia (irrelevante)",
        "resultado": {
            "titulo": "TendÃªncias de tecnologia em 2025",
            "descricao": "As principais tecnologias a dominar o mercado em 2025 incluem IA, blockchain e computaÃ§Ã£o quÃ¢ntica",
            "fonte_url": "https://example.com/tech",
            "fonte": "jina"
        },
        "query": "polÃ­ticas educacionais STEM educaÃ§Ã£o capacitaÃ§Ã£o formaÃ§Ã£o profissional",
        "num_ocorrencias": 1,
        "confiabilidade_fonte": 0.90,
        "esperado_minimo": 0.0  # Deve ser BAIXO
    }
]


async def testar_calibracao():
    """Testa a calibraÃ§Ã£o antiga vs nova"""
    print("=" * 80)
    print("TESTE DE CALIBRAÃ‡ÃƒO: ANTIGA vs NOVA")
    print("=" * 80)
    print()

    for caso in CASOS_TESTE:
        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š {caso['nome']}")
        print(f"{'=' * 80}")
        print(f"TÃ­tulo: {caso['resultado']['titulo']}")
        print(f"Query: {caso['query']}")
        print(f"OcorrÃªncias: {caso['num_ocorrencias']}")
        print(f"Fonte: {caso['resultado']['fonte']} (confiabilidade: {caso['confiabilidade_fonte']})")
        print(f"Esperado mÃ­nimo: {caso['esperado_minimo'] * 100:.0f}")
        print()

        # ANTIGA
        score_rel_old = await calcular_score_relevancia_OLD(
            f"{caso['resultado']['titulo']} {caso['resultado']['descricao']}",
            caso['query']
        )
        score_old = calcular_score_ponderado_OLD(
            caso['resultado'],
            caso['query'],
            score_rel_old,
            caso['num_ocorrencias'],
            caso['confiabilidade_fonte']
        )

        # NOVA
        texto_completo = f"{caso['resultado']['titulo']} {caso['resultado']['descricao']}"
        texto_traduzido = None
        if 'titulo_pt' in caso['resultado']:
            texto_traduzido = f"{caso['resultado'].get('titulo_pt', '')} {caso['resultado'].get('descricao_pt', '')}"

        score_rel_new = await calcular_score_relevancia_NEW(
            texto_completo,
            caso['query'],
            texto_traduzido
        )
        score_new = calcular_score_ponderado_NEW(
            caso['resultado'],
            caso['query'],
            score_rel_new,
            caso['num_ocorrencias'],
            caso['confiabilidade_fonte']
        )

        # Verificar bonus Brasil
        tem_brasil = detectar_brasil_NEW(caso['resultado'])

        # Resultados
        print(f"ğŸ”´ ANTIGA:")
        print(f"   RelevÃ¢ncia: {score_rel_old:.3f}")
        print(f"   Score Final: {score_old:.3f} = {score_old * 100:.1f}")
        print()
        print(f"ğŸŸ¢ NOVA:")
        print(f"   RelevÃ¢ncia: {score_rel_new:.3f}")
        print(f"   Score Final: {score_new:.3f} = {score_new * 100:.1f}")
        print(f"   Bonus Brasil: {'âœ… Sim' if tem_brasil else 'âŒ NÃ£o'}")
        print()

        # ComparaÃ§Ã£o
        melhoria = ((score_new - score_old) / score_old * 100) if score_old > 0 else 0
        print(f"ğŸ“ˆ Melhoria: {melhoria:+.1f}% ({score_old * 100:.1f} â†’ {score_new * 100:.1f})")

        # Verificar se atende expectativa
        if score_new >= caso['esperado_minimo']:
            print(f"âœ… PASSOU (>= {caso['esperado_minimo'] * 100:.0f})")
        else:
            print(f"âš ï¸  ABAIXO DO ESPERADO (esperado >= {caso['esperado_minimo'] * 100:.0f})")

    print(f"\n{'=' * 80}")
    print("âœ… TESTES CONCLUÃDOS")
    print(f"{'=' * 80}\n")

    # Resumo das melhorias
    print("\nğŸ“Š RESUMO DAS MELHORIAS IMPLEMENTADAS:")
    print("=" * 80)
    print("1. âœ… Score de relevÃ¢ncia: Bonus aumentado de 0.15 para 0.25")
    print("2. âœ… Matches parciais: Novo bonus de atÃ© 0.10")
    print("3. âœ… NormalizaÃ§Ã£o de ocorrÃªncias: sqrt(n)/sqrt(5) vs n/5")
    print("4. âœ… Peso de relevÃ¢ncia: Aumentado de 50% para 55%")
    print("5. âœ… Peso de ocorrÃªncias: Reduzido de 20% para 15%")
    print("6. âœ… Bonus Brasil: Novo bonus de 20% para resultados brasileiros")
    print("7. âœ… FunÃ§Ã£o de expansÃ£o: Expande scores mÃ©dios (0.3-0.8)")
    print("=" * 80)
    print()


if __name__ == "__main__":
    asyncio.run(testar_calibracao())
